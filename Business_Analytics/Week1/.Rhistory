d <- aggregate(data$price, by=list(city=data$city), FUN=range)  # temporary range
data.summary$price.range <- sapply(data.summary$city,FUN=function(x){d[d$city==x,]$x[2] - d[d$city==x,]$x[1]})
# assinging price classes
data.summary$price.mean.class <- cut(data.summary$price.mean,breaks=3)
data.summary$price.sd.class <- cut(data.summary$price.sd,breaks=2)
data.summary$price.range.class <- cut(data.summary$price.range,breaks=2)
head(data.summary)
# let's check the price distributions
#ggplot(data=data.summary, aes(x=price.mean.class,y=price.mean)) + geom_boxplot()
#ggplot(data=data.summary, aes(x=price.sd.class,y=price.sd)) + geom_boxplot()
#ggplot(data=data.summary, aes(x=price.range.class,y=price.range)) + geom_boxplot()
data.summary$lat <- sapply(data.summary$city,function(x) {as.numeric(data.tabular[data.tabular$city == x,]$lat[1])})
data.summary$lon <- sapply(data.summary$city,function(x) {as.numeric(data.tabular[data.tabular$city == x,]$lon[1])})
#ggplot(data=data.summary, aes(x=lon,y=lat,color=price.mean.class)) + geom_point(aes(shape=price.sd.class),size=4) + geom_text(aes(label=city),hjust=0, vjust=0)
data.summary$class.price[data.summary$price.mean <= 210] <- "cheap"
data.summary$class.price[data.summary$price.mean <= 284 & data.summary$price.mean > 210] <- "moderate"
data.summary$class.price[data.summary$price.mean > 284] <- "expensive"
#data.summary$class.stability[data.summary$price.sd <= 44.5] <- "stable"
#data.summary$class.stability[data.summary$price.sd > 44.5] <- "unstable"
data.summary$class.stability[data.summary$price.range <= 100] <- "stable"
data.summary$class.stability[data.summary$price.range > 100] <- "unstable"
ggplot(subset(data,city %in% data.summary$city[data.summary$class.price == "cheap"]), aes(x=date,y=price,color=city)) +
geom_point() + geom_smooth(se=FALSE) +
ggtitle("Cheap Cities") +
scale_y_continuous(limits = c(90,500))
ggplot(subset(data,city %in% data.summary$city[data.summary$class.price == "moderate"]), aes(x=date,y=price,color=city)) +
geom_point() + geom_smooth(se=FALSE) +
ggtitle("Moderate Cities") +
scale_y_continuous(limits = c(90,500))
ggplot(subset(data,city %in% data.summary$city[data.summary$class.price == "expensive"]), aes(x=date,y=price,color=city)) +
geom_point() + geom_smooth(se=FALSE) +
ggtitle("Expensive Cities") +
scale_y_continuous(limits = c(90,500))
ggplot(subset(data,city %in% data.summary$city[data.summary$class.stability == "stable"]), aes(x=date,y=price,color=city)) +
geom_point() + geom_smooth(se=FALSE) +
ggtitle("Stable Cities") +
scale_y_continuous(limits = c(90,500))
ggplot(subset(data,city %in% data.summary$city[data.summary$class.stability == "unstable"]), aes(x=date,y=price,color=city)) +
geom_point() + geom_smooth(se=FALSE) +
ggtitle("Unstable Cities") +
scale_y_continuous(limits = c(90,500))
data$trend[data$city %in% c("Phoenix","Miami")] <- "updownup"
data$trend[data$city %in% c("Boston","Meanneapolis","Chicago","New York")] <- "upupdownup"
data$trend[data$city %in% c("San Francisco")] <- "upupup"
data$trend[data$city %in% c("Seattle")] <- "upupupdown"
p1 <- ggplot(subset(data, trend=="updownup"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-down-up") +
scale_y_continuous(limits = c(90,500))
p2 <- ggplot(subset(data, trend=="upupdownup"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-up-down-up") +
scale_y_continuous(limits = c(90,500))
p3 <- ggplot(subset(data, trend=="upupup"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-up-up-up") +
scale_y_continuous(limits = c(90,500))
p4 <- ggplot(subset(data, trend=="upupupdown"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-up-up-down") +
scale_y_continuous(limits = c(90,500))
grid.arrange(p1,p2,p3,p4)
get.predictors <- function(t) {
return(data.frame(
sin = sin(pi/6*t), # 12 month period
cos = cos(pi/6*t),
sin2 = sin(pi/3*t), # 6 month period
cos2 = cos(pi/3*t)
)
)
}
get.predictions <- function(price.trend,time.end) {
lmfit <- lm(y ~ sin+cos+sin2+cos2,data=price.trend)
prediction.time <- 1:time.end
return(price.prediction <- data.frame(t=prediction.time,y=predict(lmfit,get.predictors(prediction.time))))
}
get.coeffs <- function(price.trend,time.end) {
lmfit <- lm(y ~ sin+cos+sin2+cos2,data=price.trend)
return(lmfit$coefficients)
}
p <- lapply(data.summary$city,function(the_city) {
price.train = get.predictors(1:9)
price.train$y = data$price[data$city==the_city][1:9]
price.trend = get.predictors(1:10)
price.trend$y = data$price[data$city==the_city][1:10]
price.prediction <- get.predictions(price.train,10)
ggplot(price.trend, aes(x=t,y=y)) +
geom_point() +
geom_point(data=price.prediction, aes(x=t,y=y)) +
geom_line(data=price.prediction, aes(x=t,y=y),color="red") +
geom_line() +
ggtitle(the_city)
})
do.call(grid.arrange, c(p[c(3,4,19,21,22,17)], list(ncol=3)))
do.call(grid.arrange, c(p[c(1,12,20,23)], list(ncol=2)))
# plotting the predictions
p <- lapply(data.summary$city,function(the_city) {
price.train = get.predictors(1:10)
price.train$y = data$price[data$city==the_city][1:10]
price.trend = get.predictors(1:10)
price.trend$y = data$price[data$city==the_city][1:10]
price.prediction <- get.predictions(price.train,12)
ggplot(price.trend, aes(x=t,y=y)) +
geom_point() +
geom_line(data=price.prediction, aes(x=t,y=y),color="red",size=1.4) +
geom_line() +
ggtitle(the_city)
})
do.call(grid.arrange, c(p[c(1:9)], list(ncol=3)))
do.call(grid.arrange, c(p[c(10:18)], list(ncol=3)))
do.call(grid.arrange, c(p[c(19:25)], list(ncol=3)))
data$trend[data$city %in% c("Phoenix","Miami")] <- "updownup"
data$trend[data$city %in% c("Boston","Meanneapolis","Chicago","New York")] <- "upupdownup"
data$trend[data$city %in% c("San Francisco")] <- "upupup"
data$trend[data$city %in% c("Seattle")] <- "upupupdown"
p1 <- ggplot(subset(data, trend=="updownup"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-down-up") +
scale_y_continuous(limits = c(90,500))
p2 <- ggplot(subset(data, trend=="upupdownup"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-up-down-up") +
scale_y_continuous(limits = c(90,500))
p3 <- ggplot(subset(data, trend=="upupup"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-up-up-up") +
scale_y_continuous(limits = c(90,500))
p4 <- ggplot(subset(data, trend=="upupupdown"), aes(x=date,y=price,color=city)) +
geom_point() +
geom_smooth(se=FALSE,aes(color=city)) +
ggtitle("up-up-up-down") +
scale_y_continuous(limits = c(90,500))
grid.arrange(p1,p2,p3,p4)
get.predictors <- function(t) {
return(data.frame(
sin = sin(pi/6*t), # 12 month period
cos = cos(pi/6*t),
sin2 = sin(pi/3*t), # 6 month period
cos2 = cos(pi/3*t)
)
)
}
get.predictions <- function(price.trend,time.end) {
lmfit <- lm(y ~ sin+cos+sin2+cos2,data=price.trend)
prediction.time <- 1:time.end
return(price.prediction <- data.frame(t=prediction.time,y=predict(lmfit,get.predictors(prediction.time))))
}
data
data.tabular
setwd("~/sandbox/developing-data-products/Business_Analytics/Week1")
rm(list=ls(all=TRUE))
data=read.table('DATA_2.01_SKU.csv', header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
data
str(data)
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
plot(data$CV, data$ADS, main = "SKU Example", ylab="Average Daily Sales", xlab= "Coefficient of Variation")
abline(v=0.2, col = "red") # we can draw a vertical line by using the abline function and passing it the v argument
abline(h=4, col="red") # we can draw a horizontal line by using the abline function and passing it the h argument
text(0.15,9.7, "Horses", col = "red") # we can add some text to our plot by using the text() function, here to label the group "Horses"
text(0.65,9, "Wild Bulls", col = "red") # and group "Wild Bulls"
text(0.8,2, "Crickets", col = "red") # and group "Crickets"
# Let's find groups using hierarchical clustering and check if we obtain similar results
testdata=data  # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # To keep our dataset safe, let's create a copy of it called "testdata"
summary(testdata)
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
d
head(d)
dist(1,2)
dist[1]
dist[1.2]
str(dist)
d[1]
d[2]
d[3]
d[1,2]
d[10]
head(d)
d
d[2]
d[2,1]
d[2][1]
d[100][1]
d[100][2]
d[1][100]
d[99]
d[99][2]
dummary(d)
summary(d)
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
hcward
data$groups<-cutree(hcward,k=3) # assign our points to our k=3 clusters
str(data)
install.packages("lattice") #install the lattice package by using the install.packages() function
library(lattice) # load the lattice package by using the library() function and passing it the name of the package you wish to load
xyplot(ADS~ CV,main = "After Clustering", type="p",group=groups,data=data, # define the groups to be differentiated
auto.key=list(title="Group", space = "left", cex=1.0, just = 0.95), # to produce the legend we use the auto.key= list()
par.settings = list(superpose.line=list(pch = 0:18, cex=1)), # the par.settings argument allows us to pass a list of display settings
col=c('blue','green','red')) # finally we choose the colour of our plotted points per group
rm(list=ls(all=TRUE))
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
testdata=data # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=4) # assign our points to our k=4 clusters
aggdata = aggregate(.~ groups, data=data, FUN=mean) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
aggdata = aggregate(.~ groups, data=data, FUN=mean) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
proptemp=aggregate(S~ groups, data=data, FUN=length) # we create a variable called proptemp which computes the number of observations in each group (using the S variable, but you can take any.)
aggdata$proportion=(proptemp$S)/sum(proptemp$S) # proportion of observations in each group we compute the ratio between proptemp and the total number of observations
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Let's order the groups from the larger to the smaller
# Let's see the output by calling our aggdata variable
aggdata
aggdata = aggregate(.~ groups, data=data, FUN=mean) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
aggdata
proptemp=aggregate(S~ groups, data=data, FUN=length) # we create a variable called proptemp which computes the number of observations in each group (using the S variable, but you can take any.)
aggdata$proportion=(proptemp$S)/sum(proptemp$S) # proportion of observations in each group we compute the ratio between proptemp and the total number of observations
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Let's order the groups from the larger to the smaller
testdata=data[,1:5] # we create a new dataset, called "testdata" includes all the rows and the 5 first columns of our original dataset
testdata=data[,1:5] # we create a new dataset, called "testdata" includes all the rows and the 5 first columns of our original dataset
testdata = scale(testdata) # We normalize again our original variables
d = dist(testdata, method = "euclidean") # We compute the distances between observations
hcward = hclust(d, method="ward.D") # Hiearchical Clustering using Ward criterion
data$groups = cutree(hcward,k=4) # Create segments for k=4
aggdata = aggregate(.~ groups, data=data, FUN=mean) # Aggregate the values again
proptemp=aggregate(S~ groups, data=data, FUN=length)  # Compute the number of observations per group
aggdata$proportion=(proptemp$S)/sum(proptemp$S) # Compute the proportion
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Let's order the groups from the larger to the smaller
# Let's see the output by calling our aggdata variable
aggdata
rm(list=ls(all=TRUE))
data=read.table('DATA_2.03_Telco.csv', header = T,sep=',')# The function read.table enables us to read flat files such as .csv files
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
testdata=data # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups=cutree(hcward,k=8) # assign our points to our k=8 clusters
aggdata= aggregate(.~ groups, data=data, FUN=mean) # Aggregation by group and computation of the mean values
proptemp=aggregate(Calls~ groups, data=data, FUN=length) # Computation of the number of observations by group
aggdata$proportion=(proptemp$Calls)/sum(proptemp$Calls) # Computation of the proportion by group
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Ordering from the largest group to the smallest
aggdata
data$groups= cutree(hcward,k=5) #Create segments for k=5
aggdata= aggregate(.~ groups, data=data, FUN=mean) # Aggregation by group and computation of the mean values
proptemp=aggregate(Calls~ groups, data=data, FUN=length) # Computation of the number of observations by group
aggdata$proportion=(proptemp$Calls)/sum(proptemp$Calls) # Computation of the proportion by group
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Ordering from the largest group to the smallest
aggdata
palette(rainbow(12, s = 0.6, v = 0.75)) # Select the colors to use
stars(aggdata[,2:(ncol(data))], len = 0.6, key.loc = c(11, 6),xlim=c(2,12),main = "Segments", draw.segments = TRUE,nrow = 2, cex = .75,labels=aggdata$groups)
aggdata
helo stars
help stars
stars
stars(aggdata[,2:(ncol(data))], len = 0.9, key.loc = c(11, 6), xlim=c(2,12), main = "Segments", draw.segments = TRUE,nrow = 2, cex = .75,labels=aggdata$groups)
rm(list=ls(all=TRUE))
# Let's load our dataset
data=read.table('DATA_2.01_SKU.csv', header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Let's plot our data to see if we can identify groups visually
plot(data$CV, data$ADS, main = "SKU Example", ylab="Average Daily Sales", xlab= "Coefficient of Variation")
abline(v=0.2, col = "red") # we can draw a vertical line by using the abline function and passing it the v argument
abline(h=4, col="red") # we can draw a horizontal line by using the abline function and passing it the h argument
text(0.15,9.7, "Horses", col = "red") # we can add some text to our plot by using the text() function, here to label the group "Horses"
text(0.65,9, "Wild Bulls", col = "red") # and group "Wild Bulls"
text(0.8,2, "Crickets", col = "red") # and group "Crickets"
# Let's find groups using hierarchical clustering and check if we obtain similar results
testdata=data  # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # To keep our dataset safe, let's create a copy of it called "testdata"
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups<-cutree(hcward,k=2) # assign our points to our k=3 clusters
install.packages("lattice") #install the lattice package by using the install.packages() function
library(lattice) # load the lattice package by using the library() function and passing it the name of the package you wish to load
xyplot(ADS~ CV,main = "After Clustering", type="p",group=groups,data=data, # define the groups to be differentiated
auto.key=list(title="Group", space = "left", cex=1.0, just = 0.95), # to produce the legend we use the auto.key= list()
par.settings = list(superpose.line=list(pch = 0:18, cex=1)), # the par.settings argument allows us to pass a list of display settings
col=c('blue','green','red')) # finally we choose the colour of our plotted points per group
###########################################################
####     EXAMPLE NÂ°2 - HUMAN RESSOURCES ANALYTICS      ####
###########################################################
# Set your directory to the folder where you have downloaded the HR dataset
# To clean up the memory of your current R session run the following line
rm(list=ls(all=TRUE))
# Let's load our dataset and call it data
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
xyplot(LPE~NP)
xyplot(LPE~NP, data=data)
xyplot(NP~LPE, data=data)
rm(list=ls(all=TRUE))
# Let's load our dataset and call it data
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
testdata=data # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
testdata
head(testdata)
head(testdata["S"])
head(testdata$S)
head(testdata$S)
testdata$LPE
testdata[LPE]
testdata["LPE"]
col(testdata)
testdata[1]
testdata[,1]
str(testdata)
head(testdata)
lenght(testdata)
length(testdata)
length(testdata[,1])
length(testdata[1,])
length(testdata[,1])
testdata[,1]
data$S
head(testdata)
quize.data <- data[,c(1,2,3)]
head(quize.data)
d <- dist(quize.data, method="euclidean")
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=2) # assign our points to our k=4 clusters
aggdata = aggregate(.~ groups, data=data, FUN=median) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
aggdata
# to clean up the memory of your current R session run the following line
rm(list=ls(all=TRUE))
# Let's load the data
data=read.table('DATA_2.03_Telco.csv', header = T,sep=',')# The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
testdata=data # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups=cutree(hcward,k=8) # assign our points to our k=8 clusters
aggdata= aggregate(.~ groups, data=data, FUN=mean) # Aggregation by group and computation of the mean values
proptemp=aggregate(Calls~ groups, data=data, FUN=length) # Computation of the number of observations by group
aggdata$proportion=(proptemp$Calls)/sum(proptemp$Calls) # Computation of the proportion by group
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Ordering from the largest group to the smallest
# Let's try again with 5 segments
data$groups= cutree(hcward,k=5) #Create segments for k=5
aggdata= aggregate(.~ groups, data=data, FUN=mean) # Aggregation by group and computation of the mean values
proptemp=aggregate(Calls~ groups, data=data, FUN=length) # Computation of the number of observations by group
aggdata$proportion=(proptemp$Calls)/sum(proptemp$Calls) # Computation of the proportion by group
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Ordering from the largest group to the smallest
# Let's save the output in a csv to work on it in Excel later
write.csv(aggdata, file = "aggdataTelco5seg.csv", row.names=FALSE)
# Let's draw the radar chart with the function stars()
palette(rainbow(12, s = 0.6, v = 0.75)) # Select the colors to use
stars(aggdata[,2:(ncol(data))], len = 0.9, key.loc = c(11, 6), xlim=c(2,12), main = "Segments", draw.segments = TRUE,nrow = 2, cex = .75,labels=aggdata$groups)
aggdata
aggdata= aggregate(.~ groups, data=data, FUN=length) # Aggregation by group and computation of the mean values
aggdata
rm(list=ls(all=TRUE))
# Let's load the data
data=read.table('DATA_2.03_Telco.csv', header = T,sep=',')# The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
testdata=data # To keep our dataset safe, let's create a copy of it called "testdata"
data$Calls
min(data$Calls)
min(data$Intern)
rm(list=ls(all=TRUE))
# Let's load our dataset
data=read.table('DATA_2.01_SKU.csv', header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
rm(list=ls(all=TRUE))
# Let's load our dataset and call it data
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
data
head(data)
head(data[,c("S","LPE")])
head(data[,c("S","LPE","NP")])
testdata=head(data[,c("S","LPE","NP")]) # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=2) # assign our points to our k=4 clusters
aggdata = aggregate(.~ groups, data=data, FUN=median) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
head(data[,c("S","LPE","NP")])aggdata
aggdata
aggdata = aggregate(.~ groups, data=data, FUN=median) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
testdata=head(data[,c("S","LPE","NP")]) # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=2) # assign our points to our k=4 clusters
head(data)
data=head(data[,c("S","LPE","NP")]) # To keep our dataset safe, let's create a copy of it called "testdata"
testdata=data
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=2) # assign our points to our k=4 clusters
aggdata = aggregate(.~ groups, data=data, FUN=median) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
aggdata
data=head(data[,c("S","LPE","NP")]) # To keep our dataset safe, let's create a copy of it called "testdata"
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
data=head(data[,c("S","LPE","NP")]) # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = data
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
testdata
head(testdata)
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=2) # assign our points to our k=4 clusters
aggdata = aggregate(.~ groups, data=data, FUN=median) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
aggdata
rm(list=ls(all=TRUE))
# Let's load our dataset and call it data
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
data=head(data[,c("S","LPE","NP")]) # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = data
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=2) # assign our points to our k=4 clusters
aggdata = aggregate(.~ groups, data=data, FUN=mean) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
aggdata
rm(list=ls(all=TRUE))
# Let's load our dataset and call it data
data=read.table('DATA_2.02_HR.csv',header = T,sep=',') # The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
data=head(data[,c("S","LPE","NP")]) # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = data
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups = cutree(hcward,k=2) # assign our points to our k=4 clusters
aggdata = aggregate(.~ groups, data=data, FUN=median) # The aggregate() function presents a summary of a statistic, broken down by one or more groups. Here we compute the mean of each variable for each group.
aggdata
# One thing we would like to have is the proportion of our data that is in each cluster
proptemp=aggregate(S~ groups, data=data, FUN=length) # we create a variable called proptemp which computes the number of observations in each group (using the S variable, but you can take any.)
aggdata$proportion=(proptemp$S)/sum(proptemp$S) # proportion of observations in each group we compute the ratio between proptemp and the total number of observations
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Let's order the groups from the larger to the smaller
# Let's see the output by calling our aggdata variable
aggdata
rm(list=ls(all=TRUE))
# Let's load the data
data=read.table('DATA_2.03_Telco.csv', header = T,sep=',')# The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
summary(data) # The summary() function provides for each variable in your dataset the minimum, mean, maximum and quartiles
# Now let's normalize our variables
testdata=data # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables
d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"
data$groups=cutree(hcward,k=8) # assign our points to our k=8 clusters
aggdata= aggregate(.~ groups, data=data, FUN=mean) # Aggregation by group and computation of the mean values
proptemp=aggregate(Calls~ groups, data=data, FUN=length) # Computation of the number of observations by group
aggdata$proportion=(proptemp$Calls)/sum(proptemp$Calls) # Computation of the proportion by group
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Ordering from the largest group to the smallest
# Let's try again with 5 segments
data$groups= cutree(hcward,k=5) #Create segments for k=5
aggdata= aggregate(.~ groups, data=data, FUN=length) # Aggregation by group and computation of the mean values
proptemp=aggregate(Calls~ groups, data=data, FUN=length) # Computation of the number of observations by group
aggdata$proportion=(proptemp$Calls)/sum(proptemp$Calls) # Computation of the proportion by group
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Ordering from the largest group to the smallest
# Let's save the output in a csv to work on it in Excel later
aggdata
min(data$Calls)
rm(list=ls(all=TRUE))
# Let's load the data
data=read.table('DATA_2.03_Telco.csv', header = T,sep=',')# The function read.table enables us to read flat files such as .csv files
# Now let's have a look at our variables and see some summary statistics
str(data) # The str() function shows the structure of your dataset and details the type of variables that it contains
data$Calls
summary(data$Calls)
